{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd, numpy as np, re, time\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "sns.set(style = \"whitegrid\", color_codes = True,font_scale = 1.5)\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Concatenate, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "# from Word2VecUtility import Word2VecUtility\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from collections import defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "### Figuring out the \"nan\" float values that are causing issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train-balanced-sarcasm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010826\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label             False\n",
      "comment            True\n",
      "author            False\n",
      "subreddit         False\n",
      "score             False\n",
      "ups               False\n",
      "downs             False\n",
      "date              False\n",
      "created_utc       False\n",
      "parent_comment    False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().any(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010826\n",
      "1010826\n"
     ]
    }
   ],
   "source": [
    "features = df['comment']\n",
    "labels = df['label']\n",
    "print(len(features))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      "i_count: 1010826\n",
      "bad_indices: [56269, 68590, 135348, 199910, 258718, 284331, 312969, 328775, 331735, 332600, 332631, 362293, 389792, 445204, 505371, 520619, 524263, 529336, 532823, 569280, 645450, 651242, 661519, 675235, 683899, 747602, 799033, 800812, 813274, 817886, 859333, 875251, 878050, 898863, 905291, 914178, 914615, 918700, 919882, 923678, 936221, 949593, 966886, 967116, 978220, 982492, 992907, 995023, 1001185, 1001891, 1002133, 1009303, 1010599]\n",
      "0.46416211128234863 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "counter = 0\n",
    "i_count = 0\n",
    "bad_indices = []\n",
    "\n",
    "for i,string in enumerate(features):\n",
    "    try:\n",
    "        if isinstance(string, float):\n",
    "            counter += 1\n",
    "            bad_indices.append(i_count)\n",
    "    except:\n",
    "        pass\n",
    "    i_count += 1\n",
    "\n",
    "print(counter)\n",
    "print(\"i_count:\", i_count)\n",
    "print(\"bad_indices:\", bad_indices)\n",
    "\n",
    "end = time.time()\n",
    "time_elapsed = end-start\n",
    "print(time_elapsed, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010826\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010773\n"
     ]
    }
   ],
   "source": [
    "for b in bad_indices:\n",
    "    df = df.drop([b])\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010773\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010773\n",
      "1010773\n"
     ]
    }
   ],
   "source": [
    "features = df['comment']\n",
    "labels = df['label']\n",
    "print(len(features))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "i_count 1010773\n",
      "1.845754861831665 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "counter = 0\n",
    "i_count = 0\n",
    "for i in df.itertuples():\n",
    "    if not i[2]:\n",
    "        counter += 1\n",
    "        print(counter)\n",
    "    i_count += 1\n",
    "\n",
    "print(counter)\n",
    "print(\"i_count\", i_count)\n",
    "\n",
    "end = time.time()\n",
    "time_elapsed = end-start\n",
    "print(time_elapsed, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select desired sample size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "sample_size = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(int(sample_size))\n",
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "features = df['comment']\n",
    "labels = df['label']\n",
    "print(len(features))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get volcabulary...\n",
      "get reviews_words_index...\n",
      "[[   1  797   32    3 3780    2 1026    8    2   19    2 1292]\n",
      " [   1  353   11 3782  696    3 2122   69    2    0    0    0]\n",
      " [   1  135   32    2    9   45   97 3783   35   11   77   72]\n",
      " [   1   95   10   36   38    4  536   38    4  536    0    0]\n",
      " [   1   63 2701    8 2702    8 2123   65  697 2703   25   83]\n",
      " [   1 1027    3  333   36  266   10   20    4 3785  211    2]\n",
      " [   1   71   33  698    8 1149    2    0    0    0    0    0]\n",
      " [   1 2704    3  699    9    3 2125  211    3  273   12  537]\n",
      " [   1  150   33    3  300 2126  133  354  437    9    2    6]\n",
      " [   1  575  109  538   35   15  110   54 2123   32    2 3790]\n",
      " [   1  438   10   85   13   99 3792   55    2    2    0    0]\n",
      " [   1  423   41  229   29 2705    2    0    0    0    0    0]\n",
      " [   1  111    4 1150  240  439   70   52   40  166    2    0]\n",
      " [   1  643  935   21    8  133   23  440    0    0    0    0]\n",
      " [   1  353  162    3 1029 1151    0    0    0    0    0    0]\n",
      " [   1  274 1745 2706    6    2    2    0    0    0    0    0]\n",
      " [   1  282  855  355    2   19  189  539    0    0    0    0]\n",
      " [   1    7   49   23 1152   55  458   20    4 1746  436    9]\n",
      " [   1  148   19  441   13  856   10   51    2    0    0    0]\n",
      " [   1  322  473   29  299  605    2    0    0    0    0    0]]\n",
      "(10000, 50)\n",
      "(8000, 50)\n",
      "(2000, 50)\n",
      "start training model...\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/2\n",
      "8000/8000 [==============================] - 13s 2ms/step - loss: 0.6995 - val_loss: 0.8744\n",
      "Epoch 2/2\n",
      "8000/8000 [==============================] - 11s 1ms/step - loss: 0.8733 - val_loss: 0.6903\n",
      "0.6902581949234009\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from Word2VecUtility import Word2VecUtility\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_volcabulary_and_list_words(data):\n",
    "    reviews_words = []\n",
    "    volcabulary = defaultdict(int)\n",
    "    for review in data[\"comment\"]:\n",
    "        # review_words = Word2VecUtility.review_to_wordlist(review, remove_stopwords=True)\n",
    "        review_words = review.split()\n",
    "        reviews_words.append(review_words)\n",
    "        for word in review_words:\n",
    "            volcabulary[word] += 1\n",
    "    return volcabulary, reviews_words\n",
    "\n",
    "def get_reviews_word_index(reviews_words, volcabulary, max_words, max_length):\n",
    "    volcabulary = sorted(volcabulary.items(), key = lambda x : x[1], reverse = True)[:max_words]\n",
    "    word2index = {word[0]: i for i, word in enumerate(volcabulary)}\n",
    "    reviews_words_index = [[start] + [(word2index[w] + index_from) if w in word2index else oov for w in review] for review in reviews_words]\n",
    "    # in word2vec embedding, use (i < max_words + index_from) because we need the exact index for each word, in order to map it to its vector. And then its max_words is 5003 instead of 5000.\n",
    "    # padding with 0, each review has max_length now.\n",
    "    reviews_words_index = sequence.pad_sequences(reviews_words_index, maxlen=max_length, padding='post', truncating='post')\n",
    "    return reviews_words_index\n",
    "\n",
    "data = df\n",
    "\n",
    "# data processing para\n",
    "max_words = 5000\n",
    "max_length = 50\n",
    "\n",
    "# model training parameters\n",
    "batch_size = int(0.8*len(data))//2\n",
    "embedding_dims = 100\n",
    "nb_filter = 250\n",
    "filter_length = 3\n",
    "hidden_dims = 250\n",
    "nb_epoch = 2\n",
    "\n",
    "# index trick parameters\n",
    "index_from = 3\n",
    "start = 1\n",
    "# padding = 0\n",
    "oov = 2\n",
    "\n",
    "\n",
    "print('get volcabulary...')\n",
    "volcabulary, reviews_words = get_volcabulary_and_list_words(data)\n",
    "print('get reviews_words_index...')\n",
    "reviews_words_index = get_reviews_word_index(reviews_words, volcabulary, max_words, max_length)\n",
    "\n",
    "print(reviews_words_index[:20, :12])\n",
    "print(reviews_words_index.shape)\n",
    "\n",
    "labels = data[\"label\"]\n",
    "labels[labels == 0] = 0\n",
    "labels[labels == 1] = 1\n",
    "\n",
    "pickle.dump((reviews_words_index, labels), open(\"399850by50reviews_words_index.pkl\", 'wb'))\n",
    "# (reviews_words_index, labels) = pickle.load(open(\"399850by50reviews_word2vec_words_index.pkl\", 'rb'))\n",
    "\n",
    "\n",
    "index = np.arange(reviews_words_index.shape[0])\n",
    "train_index, valid_index = train_test_split(\n",
    "    index, train_size=0.8, random_state=520)\n",
    "\n",
    "train_data = reviews_words_index[train_index]\n",
    "valid_data = reviews_words_index[valid_index]\n",
    "train_labels = labels[train_index]\n",
    "valid_labels = labels[valid_index]\n",
    "print(train_data.shape)\n",
    "print(valid_data.shape)\n",
    "\n",
    "del(labels, train_index, valid_index)\n",
    "\n",
    "print(\"start training model...\")\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_words + index_from, embedding_dims, \\\n",
    "                    input_length=max_length))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# we add a Convolution1D, which will learn nb_filter\n",
    "# word group filters of size filter_length:\n",
    "\n",
    "# filter_length is like filter size, subsample_length is like step in 2D CNN.\n",
    "model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                        filter_length=filter_length,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "# we use standard max pooling (halving the output of the previous layer):\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "\n",
    "# We flatten the output of the conv layer,\n",
    "# so that we can add a vanilla dense layer:\n",
    "model.add(Flatten())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop')\n",
    "model.fit(train_data, train_labels, batch_size=batch_size,\n",
    "          nb_epoch=nb_epoch,\n",
    "          validation_data=(valid_data, valid_labels))\n",
    "scores = model.evaluate(valid_data, valid_labels, verbose=0)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"News Headline\" Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE: https://github.com/rishabhmisra/News-Headlines-Dataset-For-Sarcasm-Detection\n",
    "# NOTES - Dataset is much cleaner, less spelling errors, higher modeling scores overall\n",
    "df = pd.read_json(\"Sarcasm_Headlines_Dataset.json\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28619\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13634\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i in df.itertuples():\n",
    "    if i[3] == 1:\n",
    "        counter += 1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use this to sample the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
       "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
       "      <td>dem rep. totally nails why congress is falling...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
       "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
       "      <td>inclement weather prevents liar from getting t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
       "      <td>mother comes pretty close to using word 'strea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_link  \\\n",
       "0  https://www.theonion.com/thirtysomething-scien...   \n",
       "1  https://www.huffingtonpost.com/entry/donna-edw...   \n",
       "2  https://www.huffingtonpost.com/entry/eat-your-...   \n",
       "3  https://local.theonion.com/inclement-weather-p...   \n",
       "4  https://www.theonion.com/mother-comes-pretty-c...   \n",
       "\n",
       "                                            headline  is_sarcastic  \n",
       "0  thirtysomething scientists unveil doomsday clo...             1  \n",
       "1  dem rep. totally nails why congress is falling...             0  \n",
       "2  eat your veggies: 9 deliciously different recipes             0  \n",
       "3  inclement weather prevents liar from getting t...             1  \n",
       "4  mother comes pretty close to using word 'strea...             1  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10200</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/this-is-u...</td>\n",
       "      <td>'this is us' is finally going to tell us how j...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10201</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/the-nfl-s...</td>\n",
       "      <td>the nfl should provide an exemption for medica...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10202</th>\n",
       "      <td>https://www.theonion.com/hes-a-stockbroker-say...</td>\n",
       "      <td>'he's a stockbroker,' says woman who finds tha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10203</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/mass-surv...</td>\n",
       "      <td>democrats and republicans agree more than you'...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10204</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/dwight-ho...</td>\n",
       "      <td>dwight howard on helping to empower and educat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10205</th>\n",
       "      <td>https://entertainment.theonion.com/1930s-comed...</td>\n",
       "      <td>1930s comedian pretty sure he's outsmarted mur...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10206</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/robin-wri...</td>\n",
       "      <td>robin wright explains why she fought for equal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10207</th>\n",
       "      <td>https://www.theonion.com/horrible-facebook-alg...</td>\n",
       "      <td>horrible facebook algorithm accident results i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10208</th>\n",
       "      <td>https://local.theonion.com/10-pound-fetus-abou...</td>\n",
       "      <td>10-pound fetus about to fucking wreck small mom</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            article_link  \\\n",
       "10200  https://www.huffingtonpost.com/entry/this-is-u...   \n",
       "10201  https://www.huffingtonpost.com/entry/the-nfl-s...   \n",
       "10202  https://www.theonion.com/hes-a-stockbroker-say...   \n",
       "10203  https://www.huffingtonpost.com/entry/mass-surv...   \n",
       "10204  https://www.huffingtonpost.com/entry/dwight-ho...   \n",
       "10205  https://entertainment.theonion.com/1930s-comed...   \n",
       "10206  https://www.huffingtonpost.com/entry/robin-wri...   \n",
       "10207  https://www.theonion.com/horrible-facebook-alg...   \n",
       "10208  https://local.theonion.com/10-pound-fetus-abou...   \n",
       "\n",
       "                                                headline  is_sarcastic  \n",
       "10200  'this is us' is finally going to tell us how j...             0  \n",
       "10201  the nfl should provide an exemption for medica...             0  \n",
       "10202  'he's a stockbroker,' says woman who finds tha...             1  \n",
       "10203  democrats and republicans agree more than you'...             0  \n",
       "10204  dwight howard on helping to empower and educat...             0  \n",
       "10205  1930s comedian pretty sure he's outsmarted mur...             1  \n",
       "10206  robin wright explains why she fought for equal...             0  \n",
       "10207  horrible facebook algorithm accident results i...             1  \n",
       "10208    10-pound fetus about to fucking wreck small mom             1  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[[i+10200 for i in range(9)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "horrible facebook algorithm accident results in exposure to new ideas\n"
     ]
    }
   ],
   "source": [
    "x = 10207\n",
    "print(df.iloc[x].loc['is_sarcastic'])\n",
    "print(df.iloc[x].loc['headline'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Headlines: 28619\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Headlines:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article_link    False\n",
      "headline        False\n",
      "is_sarcastic    False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().any(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28619\n",
      "28619\n"
     ]
    }
   ],
   "source": [
    "features = df['headline']\n",
    "labels = df['is_sarcastic']\n",
    "print(len(features))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN New Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get volcabulary...\n",
      "get reviews_words_index...\n",
      "[[   1    2  328 2966    2 2470    4  673 1062    0    0    0]\n",
      " [   1    2 2618  703 2967   46  223   12 1854 1257    9    2]\n",
      " [   1  898   35    2  660    2  592 1480    0    0    0    0]\n",
      " [   1    2 1647    2    2   15  127    3  150    0    0    0]\n",
      " [   1  468  452  289 1030    3  520  624    2    2    0    0]\n",
      " [   1   77   68    2    0    0    0    0    0    0    0    0]\n",
      " [   1  106  254    3 3990   35 2772   11  562 1576    0    0]\n",
      " [   1 1710    2    2 2968 1031   24  220   24 1522    4 1367]\n",
      " [   1 2773  285  127  136 1855    3  412    6    2 1711  345]\n",
      " [   1 2774    4  151  115   22    2    0    0    0    0    0]\n",
      " [   1   22 2471   12 3179    8  445    6 3993 1577   62  999]\n",
      " [   1    2    2  593   58  532  238   12 1258  625    2    0]\n",
      " [   1   36    3  115    2 1578    2    7 1133    0    0    0]\n",
      " [   1  521 1784  874  140   86   70    3 1712    7    2    2]\n",
      " [   1 1481 3994   13    2   26 1424    2    9    2    0    0]\n",
      " [   1 1331  359    2    2 4835   79   15   17 1579    2 2124]\n",
      " [   1   47  453 1064    2 3180    0    0    0    0    0    0]\n",
      " [   1   47   14  290   95    4   32 2333   19    2    0    0]\n",
      " [   1  533   61  433   11 3403  315    0    0    0    0    0]\n",
      " [   1 3668 2775   92   55    3 2334   21    3  387   35  522]]\n",
      "(28619, 50)\n",
      "(22895, 50)\n",
      "(5724, 50)\n",
      "start training model...\n",
      "Train on 22895 samples, validate on 5724 samples\n",
      "Epoch 1/2\n",
      "22895/22895 [==============================] - 76s 3ms/step - loss: 0.6929 - val_loss: 0.9115\n",
      "Epoch 2/2\n",
      "22895/22895 [==============================] - 76s 3ms/step - loss: 1.0213 - val_loss: 0.7042\n",
      "0.704196081554698\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from Word2VecUtility import Word2VecUtility\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_volcabulary_and_list_words(data):\n",
    "    reviews_words = []\n",
    "    volcabulary = defaultdict(int)\n",
    "    for review in data[\"headline\"]:\n",
    "        # review_words = Word2VecUtility.review_to_wordlist(review, remove_stopwords=True)\n",
    "        review_words = review.split()\n",
    "        reviews_words.append(review_words)\n",
    "        for word in review_words:\n",
    "            volcabulary[word] += 1\n",
    "    return volcabulary, reviews_words\n",
    "\n",
    "def get_reviews_word_index(reviews_words, volcabulary, max_words, max_length):\n",
    "    volcabulary = sorted(volcabulary.items(), key = lambda x : x[1], reverse = True)[:max_words]\n",
    "    word2index = {word[0]: i for i, word in enumerate(volcabulary)}\n",
    "    reviews_words_index = [[start] + [(word2index[w] + index_from) if w in word2index else oov for w in review] for review in reviews_words]\n",
    "    # in word2vec embedding, use (i < max_words + index_from) because we need the exact index for each word, in order to map it to its vector. And then its max_words is 5003 instead of 5000.\n",
    "    # padding with 0, each review has max_length now.\n",
    "    reviews_words_index = sequence.pad_sequences(reviews_words_index, maxlen=max_length, padding='post', truncating='post')\n",
    "    return reviews_words_index\n",
    "\n",
    "# data processing para\n",
    "max_words = 5000\n",
    "max_length = 50\n",
    "\n",
    "# model training parameters\n",
    "batch_size = int(0.8*len(data))//2\n",
    "embedding_dims = 100\n",
    "nb_filter = 250\n",
    "filter_length = 3\n",
    "hidden_dims = 250\n",
    "nb_epoch = 2\n",
    "\n",
    "# index trick parameters\n",
    "index_from = 3\n",
    "start = 1\n",
    "# padding = 0\n",
    "oov = 2\n",
    "\n",
    "\n",
    "data = pd.read_json(\n",
    "    'Sarcasm_Headlines_Dataset.json',lines=True)\n",
    "print('get volcabulary...')\n",
    "volcabulary, reviews_words = get_volcabulary_and_list_words(data)\n",
    "print('get reviews_words_index...')\n",
    "reviews_words_index = get_reviews_word_index(reviews_words, volcabulary, max_words, max_length)\n",
    "\n",
    "print(reviews_words_index[:20, :12])\n",
    "print(reviews_words_index.shape)\n",
    "\n",
    "labels = data[\"is_sarcastic\"]\n",
    "labels[labels == 0] = 0\n",
    "labels[labels == 1] = 1\n",
    "\n",
    "pickle.dump((reviews_words_index, labels), open(\"399850by50reviews_words_index.pkl\", 'wb'))\n",
    "# (reviews_words_index, labels) = pickle.load(open(\"399850by50reviews_word2vec_words_index.pkl\", 'rb'))\n",
    "\n",
    "\n",
    "index = np.arange(reviews_words_index.shape[0])\n",
    "train_index, valid_index = train_test_split(\n",
    "    index, train_size=0.8, random_state=520)\n",
    "\n",
    "train_data = reviews_words_index[train_index]\n",
    "valid_data = reviews_words_index[valid_index]\n",
    "train_labels = labels[train_index]\n",
    "valid_labels = labels[valid_index]\n",
    "print(train_data.shape)\n",
    "print(valid_data.shape)\n",
    "\n",
    "del(labels, train_index, valid_index)\n",
    "\n",
    "print(\"start training model...\")\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_words + index_from, embedding_dims, \\\n",
    "                    input_length=max_length))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# we add a Convolution1D, which will learn nb_filter\n",
    "# word group filters of size filter_length:\n",
    "\n",
    "# filter_length is like filter size, subsample_length is like step in 2D CNN.\n",
    "model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                        filter_length=filter_length,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "# we use standard max pooling (halving the output of the previous layer):\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "\n",
    "# We flatten the output of the conv layer,\n",
    "# so that we can add a vanilla dense layer:\n",
    "model.add(Flatten())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop')\n",
    "model.fit(train_data, train_labels, batch_size=batch_size,\n",
    "          nb_epoch=nb_epoch,\n",
    "          validation_data=(valid_data, valid_labels))\n",
    "scores = model.evaluate(valid_data, valid_labels, verbose=0)\n",
    "print(scores)\n",
    "test_predicted_labels = model.predict(features_test)\n",
    "print(classification_report(labels_test, test_predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
