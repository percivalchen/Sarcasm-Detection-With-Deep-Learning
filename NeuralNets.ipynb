{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd, numpy as np, re, time\n",
    "import itertools\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style = \"whitegrid\", color_codes = True,font_scale = 1.5)\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Concatenate, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "\n",
    "import pickle\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Reddit_Dataset_Cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010773\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010773\n",
      "1010773\n"
     ]
    }
   ],
   "source": [
    "features = df['comment']\n",
    "labels = df['label']\n",
    "print(len(features))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select desired sample size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(28619)\n",
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28619\n",
      "28619\n"
     ]
    }
   ],
   "source": [
    "features = df['comment']\n",
    "labels = df['label']\n",
    "print(len(features))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list = []\n",
    "reddit_indices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training model...\n",
      "Train on 22895 samples, validate on 5724 samples\n",
      "best_score: 0.6931684421210086\n",
      "index of best_score: [0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# from Word2VecUtility import Word2VecUtility\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_volcabulary_and_list_words(data):\n",
    "    reviews_words = []\n",
    "    volcabulary = defaultdict(int)\n",
    "    for review in data[\"comment\"]:\n",
    "        # review_words = Word2VecUtility.review_to_wordlist(review, remove_stopwords=True)\n",
    "        review_words = review.split()\n",
    "        reviews_words.append(review_words)\n",
    "        for word in review_words:\n",
    "            volcabulary[word] += 1\n",
    "    return volcabulary, reviews_words\n",
    "\n",
    "def get_reviews_word_index(reviews_words, volcabulary, max_words, max_length):\n",
    "    volcabulary = sorted(volcabulary.items(), key = lambda x : x[1], reverse = True)[:max_words]\n",
    "    word2index = {word[0]: i for i, word in enumerate(volcabulary)}\n",
    "    reviews_words_index = [[start] + [(word2index[w] + index_from) if w in word2index else oov for w in review] for review in reviews_words]\n",
    "    # in word2vec embedding, use (i < max_words + index_from) because we need the exact index for each word, in order to map it to its vector. And then its max_words is 5003 instead of 5000.\n",
    "    # padding with 0, each review has max_length now.\n",
    "    reviews_words_index = sequence.pad_sequences(reviews_words_index, maxlen=max_length, padding='post', truncating='post')\n",
    "    return reviews_words_index\n",
    "\n",
    "\n",
    "data = df\n",
    "\n",
    "# data processing para\n",
    "max_words = 5000\n",
    "max_length = 50\n",
    "\n",
    "# model training parameters\n",
    "batch_size = int(0.8*len(data))//2\n",
    "embedding_dims = 100\n",
    "nb_filter = 250\n",
    "filter_length = 3\n",
    "hidden_dims = 250\n",
    "#***** nb_epoch = 2\n",
    "\n",
    "# index trick parameters\n",
    "index_from = 3\n",
    "start = 1\n",
    "# padding = 0\n",
    "oov = 2\n",
    "\n",
    "counter=0\n",
    "best_score = 0\n",
    "for i in range(1):\n",
    "    nb_epoch=i\n",
    "\n",
    "#         print('get volcabulary...')\n",
    "    volcabulary, reviews_words = get_volcabulary_and_list_words(data)\n",
    "#         print('get reviews_words_index...')\n",
    "    reviews_words_index = get_reviews_word_index(reviews_words, volcabulary, max_words, max_length)\n",
    "\n",
    "#         print(reviews_words_index[:20, :12])\n",
    "#         print(reviews_words_index.shape)\n",
    "\n",
    "    labels = data[\"label\"]\n",
    "    labels[labels == 0] = 0\n",
    "    labels[labels == 1] = 1\n",
    "\n",
    "    pickle.dump((reviews_words_index, labels), open(\"399850by50reviews_words_index.pkl\", 'wb'))\n",
    "    # (reviews_words_index, labels) = pickle.load(open(\"399850by50reviews_word2vec_words_index.pkl\", 'rb'))\n",
    "\n",
    "\n",
    "    index = np.arange(reviews_words_index.shape[0])\n",
    "    train_index, valid_index = train_test_split(\n",
    "        index, train_size=0.8, random_state=520)\n",
    "\n",
    "    train_data = reviews_words_index[train_index]\n",
    "    valid_data = reviews_words_index[valid_index]\n",
    "    train_labels = labels[train_index]\n",
    "    valid_labels = labels[valid_index]\n",
    "#         print(train_data.shape)\n",
    "#         print(valid_data.shape)\n",
    "\n",
    "    del(labels, train_index, valid_index)\n",
    "\n",
    "    print(\"start training model...\")\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    model.add(Embedding(max_words + index_from, embedding_dims, \\\n",
    "                        input_length=max_length))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # we add a Convolution1D, which will learn nb_filter\n",
    "    # word group filters of size filter_length:\n",
    "\n",
    "    # filter_length is like filter size, subsample_length is like step in 2D CNN.\n",
    "    model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                            filter_length=filter_length,\n",
    "                            border_mode='valid',\n",
    "                            activation='relu',\n",
    "                            subsample_length=1))\n",
    "    # we use standard max pooling (halving the output of the previous layer):\n",
    "    model.add(MaxPooling1D(pool_length=2))\n",
    "\n",
    "    # We flatten the output of the conv layer,\n",
    "    # so that we can add a vanilla dense layer:\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    model.add(Dense(hidden_dims))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop')\n",
    "    model.fit(train_data, train_labels, batch_size=batch_size,\n",
    "              nb_epoch=nb_epoch,\n",
    "              validation_data=(valid_data, valid_labels))\n",
    "    scores = model.evaluate(valid_data, valid_labels, verbose=0)\n",
    "#     print(scores)\n",
    "    if scores > best_score:\n",
    "        best_score = scores\n",
    "    score_list.append(scores)\n",
    "    reddit_indices.append(counter)\n",
    "    counter+=1\n",
    "print('best_score:', max(score_list))\n",
    "print('index of best_score:', reddit_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6931684421210086]\n"
     ]
    }
   ],
   "source": [
    "print(score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"News Headline\" Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE: https://github.com/rishabhmisra/News-Headlines-Dataset-For-Sarcasm-Detection\n",
    "# NOTES - Dataset is much cleaner, less spelling errors, higher modeling scores overall\n",
    "df = pd.read_json(\"Sarcasm_Headlines_Dataset.json\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Headlines: 28619\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Headlines:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28619\n",
      "28619\n"
     ]
    }
   ],
   "source": [
    "features = df['headline']\n",
    "labels = df['is_sarcastic']\n",
    "print(len(features))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN New Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('Sarcasm_Headlines_Dataset.json',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_score_list = []\n",
    "indices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training model...\n",
      "Train on 22895 samples, validate on 5724 samples\n",
      "Epoch 1/1\n",
      "22895/22895 [==============================] - 49s 2ms/step - loss: 0.9326 - val_loss: 0.6889\n",
      "start training model...\n",
      "Train on 22895 samples, validate on 5724 samples\n",
      "Epoch 1/2\n",
      "22895/22895 [==============================] - 51s 2ms/step - loss: 0.8542 - val_loss: 0.7553\n",
      "Epoch 2/2\n",
      "22895/22895 [==============================] - 56s 2ms/step - loss: 0.7546 - val_loss: 0.7227\n",
      "start training model...\n",
      "Train on 22895 samples, validate on 5724 samples\n",
      "Epoch 1/3\n",
      "22895/22895 [==============================] - 49s 2ms/step - loss: 1.0692 - val_loss: 0.7100\n",
      "Epoch 2/3\n",
      "22895/22895 [==============================] - 60s 3ms/step - loss: 0.7138 - val_loss: 0.7077\n",
      "Epoch 3/3\n",
      "22895/22895 [==============================] - 43s 2ms/step - loss: 0.7137 - val_loss: 0.7083\n",
      "best_score: 0.7226558415978043\n",
      "index of best_score: [0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from Word2VecUtility import Word2VecUtility\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_volcabulary_and_list_words(data):\n",
    "    reviews_words = []\n",
    "    volcabulary = defaultdict(int)\n",
    "    for review in data[\"headline\"]:\n",
    "        # review_words = Word2VecUtility.review_to_wordlist(review, remove_stopwords=True)\n",
    "        review_words = review.split()\n",
    "        reviews_words.append(review_words)\n",
    "        for word in review_words:\n",
    "            volcabulary[word] += 1\n",
    "    return volcabulary, reviews_words\n",
    "\n",
    "def get_reviews_word_index(reviews_words, volcabulary, max_words, max_length):\n",
    "    volcabulary = sorted(volcabulary.items(), key = lambda x : x[1], reverse = True)[:max_words]\n",
    "    word2index = {word[0]: i for i, word in enumerate(volcabulary)}\n",
    "    reviews_words_index = [[start] + [(word2index[w] + index_from) if w in word2index else oov for w in review] for review in reviews_words]\n",
    "    # in word2vec embedding, use (i < max_words + index_from) because we need the exact index for each word, in order to map it to its vector. And then its max_words is 5003 instead of 5000.\n",
    "    # padding with 0, each review has max_length now.\n",
    "    reviews_words_index = sequence.pad_sequences(reviews_words_index, maxlen=max_length, padding='post', truncating='post')\n",
    "    return reviews_words_index\n",
    "\n",
    "# data processing para\n",
    "max_words = 5000\n",
    "max_length = 50\n",
    "\n",
    "# model training parameters\n",
    "batch_size = int(0.8*len(data))//2\n",
    "embedding_dims = 100\n",
    "nb_filter = 250\n",
    "filter_length = 3\n",
    "hidden_dims = 250\n",
    "# nb_epoch = 2\n",
    "\n",
    "# index trick parameters\n",
    "index_from = 3\n",
    "start = 1\n",
    "# padding = 0\n",
    "oov = 2\n",
    "\n",
    "counter = 0\n",
    "best_score = 0\n",
    "for i in range(1,4):\n",
    "    nb_epoch=i\n",
    "\n",
    "#         print('get volcabulary...')\n",
    "    volcabulary, reviews_words = get_volcabulary_and_list_words(data)\n",
    "#         print('get reviews_words_index...')\n",
    "    reviews_words_index = get_reviews_word_index(reviews_words, volcabulary, max_words, max_length)\n",
    "\n",
    "#         print(reviews_words_index[:20, :12])\n",
    "#         print(reviews_words_index.shape)\n",
    "\n",
    "    labels = data[\"is_sarcastic\"]\n",
    "    labels[labels == 0] = 0\n",
    "    labels[labels == 1] = 1\n",
    "\n",
    "    pickle.dump((reviews_words_index, labels), open(\"399850by50reviews_words_index.pkl\", 'wb'))\n",
    "    # (reviews_words_index, labels) = pickle.load(open(\"399850by50reviews_word2vec_words_index.pkl\", 'rb'))\n",
    "\n",
    "\n",
    "    index = np.arange(reviews_words_index.shape[0])\n",
    "    train_index, valid_index = train_test_split(\n",
    "        index, train_size=0.8, random_state=520)\n",
    "\n",
    "    train_data = reviews_words_index[train_index]\n",
    "    valid_data = reviews_words_index[valid_index]\n",
    "    train_labels = labels[train_index]\n",
    "    valid_labels = labels[valid_index]\n",
    "#         print(train_data.shape)\n",
    "#         print(valid_data.shape)\n",
    "\n",
    "    del(labels, train_index, valid_index)\n",
    "\n",
    "    print(\"start training model...\")\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    model.add(Embedding(max_words + index_from, embedding_dims, \\\n",
    "                        input_length=max_length))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # we add a Convolution1D, which will learn nb_filter\n",
    "    # word group filters of size filter_length:\n",
    "\n",
    "    # filter_length is like filter size, subsample_length is like step in 2D CNN.\n",
    "    model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                            filter_length=filter_length,\n",
    "                            border_mode='valid',\n",
    "                            activation='relu',\n",
    "                            subsample_length=1))\n",
    "    # we use standard max pooling (halving the output of the previous layer):\n",
    "    model.add(MaxPooling1D(pool_length=2))\n",
    "\n",
    "    # We flatten the output of the conv layer,\n",
    "    # so that we can add a vanilla dense layer:\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    model.add(Dense(hidden_dims))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop')\n",
    "    model.fit(train_data, train_labels, batch_size=batch_size,\n",
    "              nb_epoch=nb_epoch,\n",
    "              validation_data=(valid_data, valid_labels))\n",
    "    scores = model.evaluate(valid_data, valid_labels, verbose=0)\n",
    "    if scores > best_score:\n",
    "        best_score = scores\n",
    "    new_score_list.append(scores)\n",
    "    indices.append(counter)\n",
    "    counter+=1\n",
    "print('best_score:', max(new_score_list))\n",
    "print('index of best_score:', indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6889123063167436, 0.7226558415978043, 0.7083130971190814]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
